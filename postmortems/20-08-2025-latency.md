# Postmortem: High Latency Incident – 2025-08-20

**Service:** `search-service`

**Category:** Performance / Latency

**Date:** 2025-08-20

**Detected by:** Grafana Alert – `95% Latency > 300ms for 1m`

**Severity:** Critical

---

## 1. Timeline

| Time (UTC+6) | Event                                                                                                              |
| ------------ | ------------------------------------------------------------------------------------------------------------------ |
| 13:05        | Grafana alert fired: `95% latency > 300ms` for `search-service`.                                                   |
| 13:07        | On-call acknowledged alert. Checked Grafana dashboards → spike in p95 latency.                                     |
| 13:10        | Error logs in the k6 console `search-service` showed `Connection reset by peer` and `EOF` errors during load test. |
| 13:15        | Resource usage check (`docker stats` + cAdvisor) showed CPU at 90% for `search-service`.                           |
| 13:20        | Load test replay at lower RPS confirmed latency improved → suggested saturation point.                             |
| 13:25        | Postmortem initiated.                                                                                              |

---

## 2. Impact

- **Tenants Affected:** All tenants on `search-service`.
- **Duration:** ~1 minute (13:05–13:06).
- **User Impact:**
  - 20–30% of requests experienced > 500ms latency.
  - Some requests failed due to client-side timeouts during peak.
- **Business Impact:**
  - Search flows slowed down, potential drop in transaction completions.
  - Search-service requests queued longer due to dependency on payment results.

---

## 3. Root Cause

- Load test pushed `search-service` beyond its **single-replica capacity**, leading to:
  - CPU saturation.
  - Increased response times.
  - Higher incidence of **TCP connection resets/EOF errors** from client disconnects.
- Keep-alive/idle timeout settings were at default → exacerbated connection churn under load.
- No automatic horizontal scaling was configured; manual intervention required.

---

## 4. Corrective Actions

### Short-Term Fixes

- Increase replicas of `search-service`.
- Adjuste load test RPS to match known baseline until auto-scaling is in place.

### Long-Term Preventive Actions

1. **Baseline Recording**
   - Establish/update baseline performance metrics for payment & search services.
   - Store baselines in `baselines/` repo directory for reference.
2. **Auto-Scaling**
   - Add horizontal pod autoscaling (HPA) or Docker Swarm equivalent for `search-service`.
3. **Timeout & Keep-Alive Tuning**
   - Tune FastAPI/Uvicorn and Nginx reverse proxy keep-alive/idle timeouts.
   - Configure graceful connection handling to reduce `EOF`/`Connection reset` noise.
4. **Error Classification**
   - Differentiate between “harmless load test noise” (EOF, connection reset) vs **real bottlenecks** (5xx, CPU throttling).
5. **Load Test Policy**
   - Document safe load test RPS thresholds.
   - Run load tests against staging before production.

---
